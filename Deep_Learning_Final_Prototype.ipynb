{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import Accuracy\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,Subset,random_split\n",
    "from vit_pytorch import ViT\n",
    "from vit_pytorch.vit import Transformer, Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"filtered_species\"\n",
    "batch_size = 64 \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((256, 256)),  #Resize to minimum of all sizes - Will update size in cnn architecture\n",
    "    transforms.RandomHorizontalFlip(p = 0.25),\n",
    "    transforms.RandomRotation(degrees = 30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "subset_size = 1000\n",
    "indices = random.sample(range(len(data)), subset_size)\n",
    "data_subset = Subset(data, indices)\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "\n",
    "#change data_subset -> data for train from whole data\n",
    "training, testing = random_split(data, [train_size, test_size], generator=torch.Generator().manual_seed(1111))\n",
    "\n",
    "training_dataset = DataLoader(training, batch_size=batch_size, shuffle=True)\n",
    "testing_dataset = DataLoader(testing, batch_size=batch_size, shuffle=True)\n",
    "#For testing purposes\n",
    "#print(\"Class names:\", data.classes)\n",
    "#len(data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cnn_state=False,                # Whether to use CNN before ViT\n",
    "                 image_size=256,\n",
    "                 patch_size=16,\n",
    "                 num_class=22,\n",
    "                 dim=128,\n",
    "                 layer_count=1,\n",
    "                 head_count=1,\n",
    "                 transformer_ff_neurons=256,\n",
    "                 transformer_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn_state = cnn_state\n",
    "        self.image_size = image_size if not cnn_state else 64  # Will update if CNN is used\n",
    "\n",
    "        if cnn_state:\n",
    "            self.cnn = nn.Sequential(\n",
    "                nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),  # AlexNet-style\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                \n",
    "                nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "                nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((self.image_size, self.image_size))  # Resize to ViT input\n",
    "            )\n",
    "\n",
    "        self.vision_transformer = ViT(\n",
    "            image_size=self.image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_class,\n",
    "            dim=dim,\n",
    "            depth=layer_count,\n",
    "            heads=head_count,\n",
    "            mlp_dim=transformer_ff_neurons,\n",
    "            dropout=transformer_dropout,\n",
    "            emb_dropout=transformer_dropout,\n",
    "            channels=3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cnn_state:\n",
    "            x = self.cnn(x)\n",
    "        x = self.vision_transformer(x)\n",
    "        return x\n",
    "\n",
    "    def print_config(self):\n",
    "        print(f\"Using CNN: {self.cnn_state}\")\n",
    "        print(f\"ViT dim: {self.vision_transformer.dim}, layers: {self.vision_transformer.depth}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer_metric, accuracy_metric, device):\n",
    "    model.train()\n",
    "    net_loss = 0\n",
    "    for images, labels in tqdm(dataloader, desc = \"TRAINIGN\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        y_hat = model(images)\n",
    "        loss = criterion(y_hat, labels)\n",
    "        optimizer_metric.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_metric.step()\n",
    "        accuracy_metric.update(y_hat, labels)\n",
    "        net_loss += loss.item()\n",
    "    \n",
    "    epoch_accuracy = accuracy_metric.compute().item()\n",
    "    epoch_loss = net_loss/(len(dataloader))\n",
    "\n",
    "    return epoch_accuracy, epoch_loss\n",
    "\n",
    "def test_model(model, dataloader, criterion, accuracy_metric, device):\n",
    "    model.eval()\n",
    "    net_loss = 0\n",
    "    for images, labels in tqdm(dataloader, desc = \"TESTING\"):\n",
    "        image, label = images.to(device), labels.to(device)\n",
    "        y_hat = model(image)\n",
    "        loss = criterion(y_hat, label)\n",
    "        accuracy_metric.update(y_hat, label)\n",
    "        net_loss += loss.item()\n",
    "    \n",
    "    epoch_accuracy = accuracy_metric.compute().item()\n",
    "    epoch_loss = net_loss/(len(dataloader))\n",
    "\n",
    "    return epoch_accuracy, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINIGN:   6%|â–Œ         | 80/1336 [05:08<1:20:19,  3.84s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BirdClassifier(cnn_state=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "accuracy_score = Accuracy(task = 'multiclass', num_classes = 22).to(device)\n",
    "\n",
    "\n",
    "patience = 3\n",
    "min_delta = 1e-4\n",
    "best_accuracy = 0\n",
    "counter = 0\n",
    "\n",
    "loss_scores_train = []\n",
    "accuracy_scores_train = []\n",
    "\n",
    "for epoch in range(50):  \n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    train_acc, train_loss = train_model(model, training_dataset, criterion, optimizer, accuracy_score, device)\n",
    "    \n",
    "    accuracy_scores_train.append(train_acc)\n",
    "    loss_scores_train.append(train_loss)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    if train_acc - best_accuracy > min_delta:\n",
    "        best_accuracy = train_acc\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement. Early stopping counter: {counter}/{patience}\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.923904274965841]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_scores_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11756100505590439]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bird_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
